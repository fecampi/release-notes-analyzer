# Release Notes Analyzer

Projeto para coletar dados de Pull Requests e preparar uma entrada
estruturada para an√°lise por IA.

# ü§ñ IA Local com Ollama (LLM)

Este projeto utiliza IA rodando localmente, sem depender de APIs externas, usando o Ollama.

A LLM recomendada √© o **Llama 3.1 (8B)**, que oferece um excelente equil√≠brio entre qualidade, velocidade e consumo de recursos.

---

## üß† Modelo escolhido ‚Äî Llama 3.1 (8B)

- **Tamanho em disco:** ~4,7 GB  
- **Par√¢metros:** 8 bilh√µes  
- **Performance:** Muito boa em CPU e excelente com GPU  
- **Ideal para:**
  - Release notes
  - Resumo de issues (Jira / Git)
  - Textos em linguagem n√£o t√©cnica
  - Uso interno por times de produto e engenharia

Modelos menores tendem a gerar textos fracos, e modelos maiores exigem muito mais recursos sem ganho relevante para esse uso.

---

## üìã Requisitos

- Linux x64
- 8 GB de RAM (m√≠nimo recomendado)
- ~6 GB livres em disco
- (Opcional) GPU AMD ou NVIDIA

---

## üöÄ Instala√ß√£o do Ollama

### 1. Instalar o Ollama

```bash
curl -fsSL https://ollama.com/install.sh | sh
```

Verifique a instala√ß√£o:

```bash
ollama --version
```

---

## üì• Instalar o modelo de IA

### 2. Baixar a LLM

```bash
ollama pull llama3.1:8b
```

O download possui aproximadamente 4,7 GB.

---

### 3. Executar o modelo

```bash
ollama run llama3.1:8b
```

Teste com:

```
Explique em uma frase o que s√£o release notes para um time.
```

---

## üîå Uso via API

O Ollama sobe um servidor local automaticamente:

- URL: http://localhost:11434
- Modelo: llama3.1:8b

Exemplo de payload:

```json
{
  "model": "llama3.1:8b",
  "prompt": "Resuma essas issues em linguagem n√£o t√©cnica"
}
```

---

## üõ†Ô∏è Comandos √∫teis

```bash
ollama list
ollama rm llama3.1:8b
systemctl stop ollama
```

---

## ‚úÖ Benef√≠cios

- Dados n√£o saem da m√°quina
- Sem custo por token
- Baixa lat√™ncia
- Total controle do modelo